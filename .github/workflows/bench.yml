# Performance Benchmark Workflow
#
# Runs performance benchmarks for llm-cost and compares against tiktoken.
#
# Triggers:
#   - Weekly schedule (Sunday 00:00 UTC) - track long-term regressions
#   - Manual dispatch - for releases and investigations
#   - PR comments with "/benchmark" - on-demand for PRs
#
# Outputs:
#   - Console summary in workflow log
#   - JSON results as artifact (90 day retention)
#   - PR comment with results (when triggered by comment)
#   - Regression alert if throughput drops below threshold

name: Benchmarks

on:
  schedule:
    # Weekly on Sunday at 00:00 UTC
    - cron: '0 0 * * 0'

  workflow_dispatch:
    inputs:
      compare_tiktoken:
        description: 'Compare with tiktoken'
        required: false
        default: true
        type: boolean
      quick_mode:
        description: 'Quick mode (fewer iterations)'
        required: false
        default: false
        type: boolean
      size:
        description: 'Input size for comparison benchmark'
        required: false
        default: '1MB'
        type: choice
        options:
          - '10KB'
          - '100KB'
          - '1MB'
          - '10MB'

  issue_comment:
    types: [created]

permissions:
  contents: read
  pull-requests: write
  issues: write

env:
  ZIG_VERSION: "0.14.0"
  THROUGHPUT_THRESHOLD: "5.0"  # MB/s minimum acceptable

jobs:
  # Gate job: only run benchmarks on /benchmark comment in PRs
  check-trigger:
    runs-on: ubuntu-latest
    outputs:
      should_run: ${{ steps.check.outputs.should_run }}
      is_pr_comment: ${{ steps.check.outputs.is_pr_comment }}
    steps:
      - name: Check trigger conditions
        id: check
        run: |
          if [[ "${{ github.event_name }}" == "schedule" ]]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
            echo "is_pr_comment=false" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
            echo "is_pr_comment=false" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "issue_comment" ]]; then
            # Only run if it's a PR and comment contains /benchmark
            if [[ "${{ github.event.issue.pull_request }}" != "" ]] && \
               [[ "${{ github.event.comment.body }}" == *"/benchmark"* ]]; then
              echo "should_run=true" >> $GITHUB_OUTPUT
              echo "is_pr_comment=true" >> $GITHUB_OUTPUT
            else
              echo "should_run=false" >> $GITHUB_OUTPUT
              echo "is_pr_comment=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "should_run=false" >> $GITHUB_OUTPUT
            echo "is_pr_comment=false" >> $GITHUB_OUTPUT
          fi

  # Main benchmark job
  benchmark:
    needs: check-trigger
    if: needs.check-trigger.outputs.should_run == 'true'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11  # v4.1.1

      - name: Setup Zig
        uses: mlugg/setup-zig@v1
        with:
          version: ${{ env.ZIG_VERSION }}

      - name: Setup Python
        uses: actions/setup-python@f677139bbe7f9c59b41e40162b753c062f5d49a3  # v5.2.0
        with:
          python-version: '3.11'

      - name: Install tiktoken
        if: ${{ github.event.inputs.compare_tiktoken != 'false' }}
        run: pip install tiktoken

      - name: Build llm-cost (Release)
        run: zig build -Doptimize=ReleaseFast

      - name: Download benchmark data
        run: |
          mkdir -p data/bench

          # Generate test data of various sizes
          echo "Generating benchmark data..."

          # Small: 100 bytes
          python3 -c "
          words = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog']
          text = ' '.join(words * 20)[:100]
          print(text, end='')
          " > data/bench/small.txt

          # Medium: 10KB
          python3 -c "
          words = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog',
                   'hello', 'world', 'performance', 'benchmark', 'tokenization']
          import random
          random.seed(42)
          result = []
          size = 0
          while size < 10240:
              word = random.choice(words)
              result.append(word)
              size += len(word) + 1
          print(' '.join(result)[:10240], end='')
          " > data/bench/medium.txt

          # Large: 1MB
          python3 -c "
          words = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog',
                   'hello', 'world', 'performance', 'benchmark', 'tokenization',
                   'algorithm', 'implementation', 'optimization']
          import random
          random.seed(42)
          result = []
          size = 0
          target = 1048576
          while size < target:
              word = random.choice(words)
              result.append(word)
              size += len(word) + 1
          print(' '.join(result)[:target], end='')
          " > data/bench/large.txt

          # Pathological: 100KB of repeated 'a'
          python3 -c "print('a' * 102400, end='')" > data/bench/pathological.txt

          echo "Generated files:"
          ls -lh data/bench/

      - name: Build benchmark suite
        run: zig build bench -Doptimize=ReleaseFast

      - name: Run native benchmarks
        id: native_bench
        run: |
          echo "Running llm-cost benchmark suite..."

          # Determine mode
          if [[ "${{ github.event.inputs.quick_mode }}" == "true" ]]; then
            MODE_FLAG="--quick"
          else
            MODE_FLAG=""
          fi

          # Run benchmarks
          ./zig-out/bin/llm-cost-bench $MODE_FLAG > benchmark_results.txt 2>&1 || true

          echo "=== Native Benchmark Results ==="
          cat benchmark_results.txt

          # Also output JSON for artifacts
          ./zig-out/bin/llm-cost-bench --format=json $MODE_FLAG > benchmark_results.json 2>&1 || true

          # Extract throughput for regression check
          # This is a placeholder - adjust based on actual output format
          THROUGHPUT=$(grep -oP 'Best throughput:\s+\K[\d.]+' benchmark_results.txt || echo "0")
          echo "throughput=$THROUGHPUT" >> $GITHUB_OUTPUT

      - name: Run tiktoken comparison
        id: tiktoken_bench
        if: ${{ github.event.inputs.compare_tiktoken != 'false' }}
        run: |
          echo "Running tiktoken comparison..."

          SIZE="${{ github.event.inputs.size || '1MB' }}"
          ITERATIONS=100

          if [[ "${{ github.event.inputs.quick_mode }}" == "true" ]]; then
            ITERATIONS=20
          fi

          python scripts/bench_vs_tiktoken.py \
            --size "$SIZE" \
            --iterations $ITERATIONS \
            --encoding o200k_base \
            --output comparison.txt \
            --llm-cost ./zig-out/bin/llm-cost || true

          echo "=== Comparison Results ==="
          cat comparison.txt || echo "Comparison failed"

          # JSON output
          python scripts/bench_vs_tiktoken.py \
            --size "$SIZE" \
            --iterations $ITERATIONS \
            --json \
            --output comparison.json \
            --llm-cost ./zig-out/bin/llm-cost || true

      - name: Check for regression
        id: regression
        run: |
          THROUGHPUT="${{ steps.native_bench.outputs.throughput }}"
          THRESHOLD="${{ env.THROUGHPUT_THRESHOLD }}"

          if [[ -z "$THROUGHPUT" ]] || [[ "$THROUGHPUT" == "0" ]]; then
            echo "status=unknown" >> $GITHUB_OUTPUT
            echo "message=Could not determine throughput" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Compare using bc for floating point
          if (( $(echo "$THROUGHPUT < $THRESHOLD" | bc -l) )); then
            echo "status=regression" >> $GITHUB_OUTPUT
            echo "message=Performance regression: ${THROUGHPUT} MB/s < ${THRESHOLD} MB/s threshold" >> $GITHUB_OUTPUT
          else
            echo "status=ok" >> $GITHUB_OUTPUT
            echo "message=Performance OK: ${THROUGHPUT} MB/s (threshold: ${THRESHOLD} MB/s)" >> $GITHUB_OUTPUT
          fi

      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@65c4c4a1ddee5b72f698fdd19549f0f0fb45cf08  # v4.6.0
        with:
          name: benchmark-results-${{ github.run_number }}
          path: |
            benchmark_results.txt
            benchmark_results.json
            comparison.txt
            comparison.json
          retention-days: 90

      - name: Comment on PR
        if: needs.check-trigger.outputs.is_pr_comment == 'true'
        uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea  # v7.0.1
        with:
          script: |
            const fs = require('fs');

            let body = '## ðŸ“Š Benchmark Results\n\n';

            // Native benchmark results
            try {
              const nativeResults = fs.readFileSync('benchmark_results.txt', 'utf8');
              body += '### Native Benchmarks\n\n```\n' + nativeResults + '\n```\n\n';
            } catch (e) {
              body += '### Native Benchmarks\n\nâš ï¸ Failed to read results\n\n';
            }

            // Comparison results
            try {
              const comparison = fs.readFileSync('comparison.txt', 'utf8');
              body += '### vs tiktoken\n\n```\n' + comparison + '\n```\n\n';
            } catch (e) {
              // Comparison is optional
            }

            // Regression status
            const status = '${{ steps.regression.outputs.status }}';
            const message = '${{ steps.regression.outputs.message }}';

            if (status === 'regression') {
              body += '### âš ï¸ Regression Detected\n\n' + message + '\n';
            } else if (status === 'ok') {
              body += '### âœ… Performance Check Passed\n\n' + message + '\n';
            }

            // Post comment
            github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: body
            });

      - name: Fail on regression
        if: steps.regression.outputs.status == 'regression'
        run: |
          echo "::error::${{ steps.regression.outputs.message }}"
          exit 1

  # Store benchmark history (only on main branch)
  store-history:
    needs: benchmark
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && github.event_name != 'issue_comment'

    steps:
      - name: Checkout repository
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11

      - name: Download benchmark results
        uses: actions/download-artifact@fa0a91b85d4f404e444e00e005971372dc801d16  # v4.1.8
        with:
          name: benchmark-results-${{ github.run_number }}

      - name: Append to history
        run: |
          DATE=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          COMMIT="${{ github.sha }}"
          SHORT_SHA=$(echo $COMMIT | cut -c1-7)

          # Extract metrics from JSON if available
          if [[ -f benchmark_results.json ]]; then
            # Parse with jq if available, otherwise use grep
            if command -v jq &> /dev/null; then
              THROUGHPUT=$(jq -r '.benchmarks[-1].throughput_mbps // 0' benchmark_results.json)
            else
              THROUGHPUT=$(grep -oP '"throughput_mbps":\s*\K[\d.]+' benchmark_results.json | tail -1 || echo "0")
            fi
          else
            THROUGHPUT="0"
          fi

          # Create or append to history CSV
          if [[ ! -f benchmark_history.csv ]]; then
            echo "date,commit,throughput_mbps" > benchmark_history.csv
          fi

          echo "$DATE,$SHORT_SHA,$THROUGHPUT" >> benchmark_history.csv

          echo "Recorded: $DATE, $SHORT_SHA, $THROUGHPUT MB/s"

      - name: Upload history
        uses: actions/upload-artifact@65c4c4a1ddee5b72f698fdd19549f0f0fb45cf08
        with:
          name: benchmark-history
          path: benchmark_history.csv
          retention-days: 365
