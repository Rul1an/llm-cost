# v0.4 Technical Implementation Plan

**Focus**: Multi-provider readiness, Agentic UX, FinOps Rigor.
**Status**: Ready for Implementation.
**Toolchain**: Zig 0.13.x.

---

## Phase 0: Freeze v0.3 Invariants

Before building v0.4, we lock down the non-negotiables:
1.  **Parity**: `o200k_base` / `cl100k_base` must remain bit-exact to `tiktoken` (verified by Evil Corpus v2).
2.  **Performance**: Worst-case BPE merge is O(N log N). `a * 4096` < 2ms.
3.  **Safety**: Fuzzing (`zig build fuzz`) remains panic-free.

---

## Epic 1: Registry V2 & ModelSpec

**Objective**: Move from "encoding-centric" to "model-centric" resolution.

### 1.1 New Core Types
**Location**: `src/tokenizer/model_registry.zig` (New File)

```zig
pub const AccuracyTier = enum {
    exact,      // Proven bit-identical (parity tests)
    family,     // Same tokenizer family, no strict proof yet
    heuristic,  // Generic whitespace/char estimate
};

pub const Provider = enum {
    openai,
    meta,
    mistral,
    generic,
};

pub const ModelSpec = struct {
    provider: Provider,
    canonical_model: []const u8, // e.g. "openai/gpt-4o"
    display_name: []const u8,    // e.g. "gpt-4o"
    encoding: ?EncodingSpec,     // null for heuristic
    accuracy: AccuracyTier,
    has_pricing: bool,
};
```

### 1.2 Resolution Logic
**Function**: `ModelRegistry.resolve(name: []const u8) ?ModelSpec`

1.  **Namespaced**: Check for `provider/model` (e.g., `openai/gpt-4o`).
2.  **Alias**: Map short names `gpt-4o` -> `openai/gpt-4o`.
3.  **Fallback**: `foo/bar` -> `provider=generic`, `accuracy=heuristic`.

**Tests (`src/test/model_registry_test.zig`)**:
- `openai/gpt-4o` -> `exact`, `o200k_base`.
- `gpt-4o` -> same as above.
- `meta/llama-3-8b` -> `family` (if mapped to cl100k) or `heuristic` (initial).
- `unknown/model` -> `heuristic`, `encoding=null`.

### 1.3 Migration & Layering
- `tokenizer/registry.zig`: Holds **only** raw `EncodingSpec` data.
- `tokenizer/model_registry.zig`: Imports `registry.zig`, defines `ModelSpec`.
- `pricing.zig`: Keyed by `canonical_model` string.

---

## Epic 2: Accuracy in Output & CLI UX

**Objective**: Explicit confidence labeling in all machine-readable output.

### 2.1 Struct Updates
**Location**: `src/cli/types.zig`
```zig
pub const TokenCountResult = struct {
    model: []const u8,      // canonical name
    encoding: []const u8,   // encoding name or "heuristic"
    accuracy: []const u8,   // "exact", "family", "heuristic"
    tokens_input: u64,
    // ...
};
```

### 2.2 JSON Enrichment
Update `tokens`, `price`, and `pipe` commands to serialize the `accuracy` field.

**Tests (`src/cli/tests/output_accuracy.zig`)**:
- Golden JSON check: `--model gpt-4o` output must contain `"accuracy": "exact"`.
- Golden JSON check: `--model generic/foo` output must contain `"accuracy": "heuristic"`.

---

## Epic 3: Pipe Summary & Quota Guard (Agent/Batch UX)

**Objective**: Streaming aggregation and strict budget enforcement.

### 3.1 Data Structures
**Location**: `src/cli/pipe.zig` / `src/cli/summary.zig`

```zig
pub const PipeSummary = struct {
    rows: u64,
    tokens_in: u64,
    tokens_out: u64,
    cost_usd_micros: u64, // accumulated as integer
};

pub const PipeQuota = struct {
    max_tokens: ?u64,
    max_cost_usd: ?f64,
};
```

### 3.2 Strategy: Single-Worker Quota
To avoid complex synchronization/race conditions in v0.4:
- If `--max-tokens` or `--max-cost` is present -> **Force `workers=1`**.
- In loop: Check quota -> Process Line -> Update Summary.
- If quota exceeded: Stop loop, exit with `error.QuotaExceeded` (or custom exit code).

### 3.3 CLI Flags
- `--summary`: Print aggregated stats to stderr at end.
- `--max-tokens N`: Stop after N total tokens.
- `--max-cost N`: Stop after N USD.

**Tests**:
- Run pipe with inputs > max_tokens. Assert exit code != 0 and summary shows processed <= max.

---

## Epic 4: Multi-encoding Groundwork

**Objective**: Prepare for Llama/Mistral without breaking v0.3 stability.

### 4.1 Family Definitions
Extend `ModelSpec`:
```zig
pub const Family = enum {
    openai_o200k,
    openai_cl100k,
    llama_cl100k_like, // Tiktoken-based, diff vocab
    tekken_like,       // Mistral
    whitespace,
};
```

### 4.2 Initial Mappings
- `meta/llama-3-*`: Map to `cl100k` engine if behavior matches, but construct spec with `accuracy=family`.
- `mistral/*`: Map to `heuristic` for now (until Tekken engine implemented).

---

## Epic 5: Quality Assurance

### 5.1 Fuzzing
- Extend `fuzz_test.zig` to fuzz `ModelRegistry.resolve(random_bytes)`.
- Ensure no panic on malformed namespaced strings.

### 5.2 Performance Sanity
- Run `zig build bench-bpe`.
- Run pipe benchmark (multi-thread vs single-thread quota) to ensure no regression in `workers=1` baseline.

---

## Epic 6: Documentation & Release

### 6.1 Documentation
- **README**: New "Accuracy" matrix (Exact vs Family vs Heuristic).
- **v0.4-spec**: Document how to add custom vocabs via `tools/convert_vocab.zig`.

### 6.2 CI
- Add `test-cli` step if possible (invoking built binary).
- Ensure `verify` job runs all new registry tests.

---

## Summary check
- **Circular Imports**: Mitigated by strict separation of Data (`registry`) and Logic (`model_registry`).
- **Data Races**: Mitigated by restricting Quota Guards to single-thread.
- **Ambiguity**: Mitigated by explicit `accuracy` field everywhere.

Ready to execute Epic 1.
